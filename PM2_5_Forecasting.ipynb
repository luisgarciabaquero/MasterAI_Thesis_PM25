{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map with stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import folium\n",
    "e = json.load(open('world-countries.json','r')) #Load json with world countries data (shapefiles)\n",
    "json.dump(e['features'][73], open('india.json','w')) #Obtain outlined map of India\n",
    "\n",
    "\n",
    "stations_coords = pd.read_csv('station_coords.csv', sep=';') #Load dataset with coordinates of stations\n",
    "stations_coords_filtered = stations_coords.loc[stations_coords.Lat != '-']\n",
    "\n",
    "folium_map = folium.Map(width = '60%',height=800,location=[20, 77], #Initialize map\n",
    "                        zoom_start=5,\n",
    "                        tiles=\"Stamen Terrain\",min_lat=7, max_lat=35, min_lon=73, max_lon=90)\n",
    "for x in stations_coords_filtered.iterrows(): #Plot a circle per station of the dataset\n",
    "    name = x[0]\n",
    "    lat, lon = x[1]['Lat'], x[1]['Long']\n",
    "    folium.CircleMarker([lat, lon], radius=5, color='#000000',fill_color='#D3D3D3' , fill_opacity=1).add_to(folium_map)\n",
    "\n",
    "folium.GeoJson('india.json').add_to(folium_map) #Add India map\n",
    "\n",
    "folium_map.save(\"map.html\") #Save map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_hour = pd.read_csv('station_hour.csv') #Load hourly measurements of stations\n",
    "stations_info = pd.read_csv('stations.csv') #Load dataset with stations' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stations = pd.DataFrame() #Initialize dataset\n",
    "\n",
    "for stat in uni_stations: #Loop through all stations\n",
    "    station_id = stations_info.loc[stations_info.City == stat, 'StationId'] #Subset of data for the specific station\n",
    "    \n",
    "    stations_hour_filtered = stations_hour.loc[stations_hour.StationId == station_id.values[0]]\n",
    "    stations_hour_filtered.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    if stations_hour_filtered.shape[0] == 0: #If the station is empty, go to the next\n",
    "        next\n",
    "    \n",
    "    #Read weather data corresponding to the city where the station is\n",
    "    weat_stat = pd.read_csv('weather/' + stat.replace(' ', '+') + ',India.csv')[['date_time', 'DewPointC',\n",
    "                                                                                'cloudcover', 'humidity',\n",
    "                                                                               'precipMM', 'pressure',\n",
    "                                                                               'tempC', 'windspeedKmph']]\n",
    "    weat_stat_final = weat_stat[['date_time']]\n",
    "    \n",
    "    variables_list = []\n",
    "    \n",
    "    #Shifted weather variables\n",
    "    \n",
    "    for period in [24, 48, 72, 168, 336]: #Loop through lags\n",
    "        aux_weat_stat = weat_stat.drop('date_time', axis = 1).shift(period) #Apply lag\n",
    "        aux_weat_stat.columns = ['DewPointC_td' + str(period), 'cloudcover_td'  + str(period),\n",
    "                                 'humidity_td'  + str(period), 'precipMM_td'  + str(period),\n",
    "                                'pressure_td'  + str(period), 'tempC_td'  + str(period),\n",
    "                                 'windspeedKmph_td'  + str(period)]\n",
    "        \n",
    "        aux_weat_stat.drop(['cloudcover_td'  + str(period), 'precipMM_td'  + str(period),\n",
    "                           'tempC_td'  + str(period)], axis = 1, inplace = True)\n",
    "        \n",
    "        variables_list = variables_list + list(aux_weat_stat.columns)\n",
    "        \n",
    "        weat_stat_final = pd.concat((weat_stat_final, aux_weat_stat), axis = 1) #Add to dataset\n",
    "\n",
    "    #Rolling average of weather variables (except precipitation)\n",
    "    \n",
    "    for period in [24, 48, 72, 168, 336]: #Loop through time window widths\n",
    "        aux_weat_stat = weat_stat.drop(['precipMM', 'date_time'], axis = 1).rolling(period, min_periods = 24).mean() #Compute average on the window\n",
    "        aux_weat_stat.columns = ['DewPointC_tro' + str(period), 'cloudcover_tro' + str(period),\n",
    "                                 'humidity_tro' + str(period), 'pressure_tro' + str(period),\n",
    "                                 'tempC_tro' + str(period), 'windspeedKmph_tro' + str(period)]\n",
    "        \n",
    "        aux_weat_stat.drop(['tempC_tro'  + str(period)], axis = 1, inplace = True)\n",
    "        \n",
    "        variables_list = variables_list + list(aux_weat_stat.columns)\n",
    "        \n",
    "        weat_stat_final = pd.concat((weat_stat_final, aux_weat_stat.shift(24)), axis = 1)\n",
    "\n",
    "    #Rolling cumulated sum of precipitations\n",
    "\n",
    "    for period in [24, 48, 72, 168, 336]:\n",
    "        aux_weat_stat = weat_stat[['precipMM']].rolling(period, min_periods = 24).sum() #Compute cumulative sum in the window\n",
    "        aux_weat_stat.columns = ['precipMM_tprep' + str(period)]\n",
    "        \n",
    "        variables_list = variables_list + list(aux_weat_stat.columns)\n",
    "        \n",
    "        weat_stat_final = pd.concat((weat_stat_final, aux_weat_stat.shift(24)), axis = 1)\n",
    "    \n",
    "    #Lagged air quality variables\n",
    "    \n",
    "    #all compounds: 'PM2.5','PM10','NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene'\n",
    "    used_compounds = ['PM2.5', 'NOx', 'SO2', 'O3']\n",
    "    stations_hour_filtered_aux = stations_hour_filtered[['Datetime'] + used_compounds]\n",
    "    \n",
    "    stations_hour_filtered_aux_final = stations_hour_filtered_aux[['Datetime']]\n",
    "    \n",
    "    for period in [24, 48, 72, 168, 336]:\n",
    "        aux_stat_filtered = stations_hour_filtered_aux.drop('Datetime', axis = 1).shift(period)\n",
    "        \n",
    "        used_compounds_columns = [x + '_td' + str(period) for x in used_compounds]\n",
    "        aux_stat_filtered.columns = used_compounds_columns\n",
    "        \n",
    "        variables_list = variables_list + list(aux_stat_filtered.columns)\n",
    "        \n",
    "        stations_hour_filtered_aux_final = pd.concat((stations_hour_filtered_aux_final, aux_stat_filtered), axis = 1)\n",
    "\n",
    "    #Rolling average of weather variables (except precipitation)\n",
    "    \n",
    "    for period in [24, 48, 72, 168, 336]:\n",
    "        aux_stat_filtered = stations_hour_filtered_aux.drop(['Datetime'], axis = 1).rolling(period, min_periods = 24).mean()\n",
    "        used_compounds_columns = [x + '_tro' + str(period) for x in used_compounds]\n",
    "        aux_stat_filtered.columns = used_compounds_columns\n",
    "        \n",
    "        variables_list = variables_list + list(aux_stat_filtered.columns)\n",
    "        stations_hour_filtered_aux_final = pd.concat((stations_hour_filtered_aux_final, aux_stat_filtered.shift(24)), axis = 1)\n",
    "\n",
    "    stations_hour_filtered_weather = stations_hour_filtered.merge(weat_stat_final, how = 'inner',\n",
    "                                                                 left_on = 'Datetime',\n",
    "                                                                 right_on = 'date_time')\n",
    "    \n",
    "    stations_hour_filtered_weather = stations_hour_filtered_weather.merge(stations_hour_filtered_aux_final, how = 'inner',\n",
    "                                                                 left_on = 'Datetime',\n",
    "                                                                 right_on = 'Datetime')\n",
    "    \n",
    "    stations_hour_filtered_weather['Datetime'] = pd.to_datetime(stations_hour_filtered_weather['Datetime'])\n",
    "    \n",
    "    stations_hour_filtered_weather['DoW_sin'] = np.sin(2*np.pi*stations_hour_filtered_weather['Datetime'].dt.dayofweek/7)\n",
    "    stations_hour_filtered_weather['DoW_cos'] = np.cos(2*np.pi*stations_hour_filtered_weather['Datetime'].dt.dayofweek/7)\n",
    "\n",
    "    #stations_hour_filtered_weather['Mon'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 0))\n",
    "    #stations_hour_filtered_weather['Tue'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 1))\n",
    "    #stations_hour_filtered_weather['Wed'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 2))\n",
    "    #stations_hour_filtered_weather['Thu'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 3))\n",
    "    #stations_hour_filtered_weather['Fri'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 4))\n",
    "    #stations_hour_filtered_weather['Sat'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 5))\n",
    "    #stations_hour_filtered_weather['Sun'] = stations_hour_filtered_weather['DoW'].apply(lambda x: int(x == 6))\n",
    "    \n",
    "    #stations_hour_filtered_weather.drop('DoW', inplace = True, axis = 1)\n",
    "    \n",
    "    #variables_list = variables_list + ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat','Sun']\n",
    "    \n",
    "    variables_list = variables_list + ['DoW_sin', 'DoW_cos']\n",
    "    \n",
    "    stations_hour_filtered_weather['Month_sin'] = np.sin(2*np.pi*stations_hour_filtered_weather['Datetime'].dt.month/12)\n",
    "    stations_hour_filtered_weather['Month_cos'] = np.cos(2*np.pi*stations_hour_filtered_weather['Datetime'].dt.month/12)\n",
    "\n",
    "    #stations_hour_filtered_weather['Jan'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 1))\n",
    "    #stations_hour_filtered_weather['Feb'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 2))\n",
    "    #stations_hour_filtered_weather['Mar'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 3))\n",
    "    #stations_hour_filtered_weather['Apr'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 4))\n",
    "    #stations_hour_filtered_weather['May'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 5))\n",
    "    #stations_hour_filtered_weather['Jun'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 6))\n",
    "    #stations_hour_filtered_weather['Jul'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 7))\n",
    "    #stations_hour_filtered_weather['Aug'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 8))\n",
    "    #stations_hour_filtered_weather['Sep'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 9))\n",
    "    #stations_hour_filtered_weather['Oct'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 10))\n",
    "    #stations_hour_filtered_weather['Nov'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 11))\n",
    "    #stations_hour_filtered_weather['Dec'] = stations_hour_filtered_weather['Month'].apply(lambda x: int(x == 12))\n",
    "    #\n",
    "    #stations_hour_filtered_weather.drop('Month', inplace = True, axis = 1)\n",
    "    \n",
    "    variables_list = variables_list + ['Month_sin', 'Month_cos']\n",
    "    \n",
    "    stations_hour_filtered_weather['Hour_sin'] = np.sin(2*np.pi*stations_hour_filtered_weather['Datetime'].dt.hour/24)\n",
    "    stations_hour_filtered_weather['Hour_cos'] = np.cos(2*np.pi*stations_hour_filtered_weather['Datetime'].dt.hour/24)\n",
    "\n",
    "    variables_list = variables_list + ['Hour_sin', 'Hour_cos']\n",
    "    \n",
    "    full_stations = pd.concat((full_stations, stations_hour_filtered_weather), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stations[['PM2.5'] + variables_list].dropna().corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.groupby('StationId').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = full_stations[['StationId', 'date_time', 'PM2.5'] + variables_list].dropna()\n",
    "final_data.to_csv('final_pm25_3d_2w.csv', index = False) #Save Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_info = pd.read_csv('stations.csv')\n",
    "uni_stations = list(stations_info.City.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.DataFrame()\n",
    "\n",
    "for stat in uni_stations:\n",
    "    weat_stat = pd.read_csv('weather/' + stat.replace(' ', '+') + ',India.csv')[['date_time', 'DewPointC',\n",
    "                                                                                'cloudcover', 'humidity',\n",
    "                                                                               'precipMM', 'pressure',\n",
    "                                                                               'tempC', 'windspeedKmph']]\n",
    "    \n",
    "    weather_data = pd.concat((weather_data, weat_stat), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, sharey  = True)\n",
    "\n",
    "ax1.hist(weather_data.DewPointC,20, density = True)\n",
    "ax2.hist(weather_data.cloudcover,20, density = True)\n",
    "ax3.hist(weather_data.humidity,20, density = True)\n",
    "ax4.hist(weather_data.pressure,20, density = True)\n",
    "ax5.hist(weather_data.tempC,20, density = True)\n",
    "ax6.hist(weather_data.windspeedKmph,20, density = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particulate matter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_hour = pd.read_csv('station_hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_hour.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_hour.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = stations_hour.sample(1000000)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey  = True)\n",
    "\n",
    "ax2.hist(sam['O3'].dropna(), 40, density = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_pm25_3d_2w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = list(set(list(data.columns)) - set(['StationId', 'date_time', 'PM2.5']))\n",
    "y_var = ['PM2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vars_cats = defaultdict(list)\n",
    "for key, value in [(x[0:3].split('_')[0], x) for x in x_vars]:\n",
    "    vars_cats[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_cats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data.loc[:, x_vars]\n",
    "y = data.loc[:, y_var]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=14)\n",
    "    \n",
    "# define random forest classifier\n",
    "forest = RandomForestRegressor(n_jobs=-1, max_depth=5, verbose = 0)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(forest, n_estimators='auto', verbose=2, random_state=14)\n",
    "\n",
    "# find all relevant features\n",
    "feat_selector.fit(x_train.values, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_vars[i] for i in range(len(x_vars)) if feat_selector.support_[i] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ranks = list(zip(x_train.columns, \n",
    "                         feat_selector.ranking_, \n",
    "                         feat_selector.support_))\n",
    "\n",
    "# iterate through and print out the results\n",
    "for feat in feature_ranks:\n",
    "    print('Feature: {:<25} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = set(data.columns)\n",
    "\n",
    "new_data = data[list(data_n - set(['Hour_sin', 'Hour_cos', 'DoW_sin', 'DoW_cos', 'Month_sin', \n",
    "                                   'O3_td24','O3_td48','O3_td72','O3_td168','O3_td336',\n",
    "                                  'SO2_td24','SO2_td48','SO2_td72','SO2_td168','SO2_td336']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('final_pm25_3days_2w_fs.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_pm25_fs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[list(set(data.columns) - set(['date_time', 'StationId', 'PM2.5']))]\n",
    "y = data[['PM2.5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.loc[data.date_time < '2019-07-01']\n",
    "data_val = data.loc[(data.date_time >= '2019-07-01') & (data.date_time < '2020-01-01')]\n",
    "data_test = data.loc[(data.date_time >= '2020-01-01')]\n",
    "\n",
    "X_train = data_train[list(set(data.columns) - set(['date_time', 'StationId', 'PM2.5']))]\n",
    "y_train = data_train[['PM2.5']]\n",
    "\n",
    "X_val = data_val[list(set(data.columns) - set(['date_time', 'StationId', 'PM2.5']))]\n",
    "y_val = data_val[['PM2.5']]\n",
    "\n",
    "X_test = data_test[list(set(data.columns) - set(['date_time', 'StationId', 'PM2.5']))]\n",
    "y_test = data_test[['PM2.5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tr = reg.predict(X_train)\n",
    "predictions_test = reg.predict(np.concatenate((X_test, X_val), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(np.concatenate((y_test, y_val), axis = 0), predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Normalizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train = transformer.transform(X_train)\n",
    "\n",
    "transformed_X_val = transformer.transform(X_val)\n",
    "\n",
    "transformed_X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(transformed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tr = reg.predict(transformed_X_train)\n",
    "predictions_test = reg.predict(np.concatenate((transformed_X_test, transformed_X_val), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(np.concatenate((y_test, y_val), axis = 0), predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train = transformer.transform(X_train)\n",
    "\n",
    "transformed_X_val = transformer.transform(X_val)\n",
    "\n",
    "transformed_X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(transformed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tr = reg.predict(transformed_X_train)\n",
    "predictions_test = reg.predict(np.concatenate((transformed_X_test, transformed_X_val), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(np.concatenate((y_test, y_val), axis = 0), predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(np.concatenate((y_test, y_val), axis = 0), predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha_iter = -300\n",
    "best_error = 10000\n",
    "for alpha_iter in np.logspace(-4, 0, 500):\n",
    "    reg = linear_model.Ridge(alpha = alpha_iter, normalize = True)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    predictions_val = reg.predict(X_val)\n",
    "    predictions_error = sklearn.metrics.mean_squared_error(y_val, predictions_val)\n",
    "    \n",
    "    if predictions_error < best_error:\n",
    "        best_error = predictions_error\n",
    "        best_alpha_iter = alpha_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.Ridge(alpha = best_alpha_iter, normalize = True)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tr = reg.predict(X_train)\n",
    "predictions_test = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Normalizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train = transformer.transform(X_train)\n",
    "\n",
    "transformed_X_val = transformer.transform(X_val)\n",
    "\n",
    "transformed_X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha_iter = -300\n",
    "best_error = 10000\n",
    "for alpha_iter in np.logspace(-4, 0, 500):\n",
    "    reg = linear_model.Ridge(alpha = alpha_iter, normalize = True)\n",
    "    reg.fit(transformed_X_train, y_train)\n",
    "    \n",
    "    predictions_val = reg.predict(transformed_X_val)\n",
    "    predictions_error = sklearn.metrics.mean_squared_error(y_val, predictions_val)\n",
    "    \n",
    "    if predictions_error < best_error:\n",
    "        best_error = predictions_error\n",
    "        best_alpha_iter = alpha_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.Ridge(alpha = best_alpha_iter)\n",
    "reg.fit(transformed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tr = reg.predict(transformed_X_train)\n",
    "predictions_test = reg.predict(transformed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import space\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the space of hyperparameters to search\n",
    "search_space = list()\n",
    "search_space.append(space.space.Real(1e-6, 100.0, 'log-uniform', name='C'))\n",
    "search_space.append(space.space.Categorical(['linear', 'poly', 'rbf', 'sigmoid'], name='kernel'))\n",
    "search_space.append(space.space.Integer(1, 5, name='degree'))\n",
    "search_space.append(space.space.Real(1e-6, 100.0, 'log-uniform', name='gamma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function used to evaluate a given configuration\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.svm import SVR\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_sampled = X_train\n",
    "y_train_sampled = y_train\n",
    "\n",
    "X_val_sampled = X_val\n",
    "y_val_sampled = y_val\n",
    "\n",
    "transformer = StandardScaler().fit(X_train_sampled)\n",
    "\n",
    "transformed_X_train = transformer.transform(X_train_sampled)\n",
    "transformed_X_val = transformer.transform(X_val_sampled)\n",
    "transformed_X_test = transformer.transform(X_test)\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def evaluate_model(**params):\n",
    "    # configure the model with specific hyperparameters\n",
    "    model = SVR()\n",
    "    model.set_params(**params)\n",
    "    # calculate 5-fold cross validation\n",
    "    model.fit(transformed_X_train, y_train_sampled.to_numpy().ravel())\n",
    "    predictions = model.predict(transformed_X_val)\n",
    "    result = sklearn.metrics.mean_squared_error(y_val_sampled, predictions)\n",
    "    # convert from a maximizing score to a minimizing score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gp_minimize(evaluate_model, search_space, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.x #without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR()\n",
    "model.set_params(kernel = 'poly', degree = 5, gamma = 1e-06, C = 100)\n",
    "# calculate 5-fold cross validation\n",
    "model.fit(transformed_X_train, y_train_sampled.to_numpy().ravel())\n",
    "predictions = model.predict(transformed_X_val)\n",
    "sklearn.metrics.mean_absolute_error(y_val_sampled, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "svm_params = {'C': 3.42860186,\n",
    "                 'gamma': 0.014033215,\n",
    "                  'ncomp': 352,\n",
    "                 'kernel': 'rbf'}\n",
    "    \n",
    "model = LinearSVR(random_state = 42, max_iter = 5000, C = svm_params['C'], dual = False, loss = 'squared_epsilon_insensitive')\n",
    "\n",
    "feature_map_nystroem = Nystroem(gamma= svm_params['gamma'],\n",
    "                                kernel = svm_params['kernel'],\n",
    "                             random_state=1,\n",
    "                             n_components= svm_params['ncomp'])\n",
    "\n",
    "data_train_transformed = feature_map_nystroem.fit_transform(np.concatenate((transformed_X_train, transformed_X_val), axis = 0))\n",
    "\n",
    "model.fit(data_train_transformed, np.concatenate((y_train_sampled.to_numpy().ravel(), y_val_sampled.to_numpy().ravel()), axis = 0))\n",
    "\n",
    "data_test_transformed = feature_map_nystroem.transform(transformed_X_test)\n",
    "predictions = model.predict(data_test_transformed)\n",
    "predictions_error = sklearn.metrics.mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(30, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "best_solution = 10000\n",
    "\n",
    "X_train_sample = X_train[0:20000]\n",
    "y_train_sample = y_train[0:20000]\n",
    "\n",
    "for i in range(0,100):\n",
    "    \n",
    "    rf_params = {'n_estimators': random.choice(n_estimators),\n",
    "                 'max_features':random.choice(max_features),\n",
    "                 'max_depth': random.choice(max_depth),\n",
    "                 'min_samples_split': random.choice(min_samples_split),\n",
    "                 'min_samples_leaf': random.choice(min_samples_leaf),\n",
    "                 'bootstrap': random.choice(bootstrap)\n",
    "    }\n",
    "\n",
    "    print(rf_params)\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state = 42, n_jobs = -1)\n",
    "    rf.set_params(**rf_params)\n",
    "    rf.fit(X_train_sample, y_train_sample.to_numpy().ravel())\n",
    "    \n",
    "    predictions = rf.predict(X_val)\n",
    "    \n",
    "    predictions_error = sklearn.metrics.mean_absolute_error(y_val, predictions)\n",
    "\n",
    "    if predictions_error < best_solution:\n",
    "        best_params = rf_params\n",
    "        best_solution = predictions_error\n",
    "        \n",
    "        print(best_solution)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {'n_estimators': 500,\n",
    "                 'max_features': 'sqrt',\n",
    "                 'max_depth': 60,\n",
    "                 'min_samples_split': 2,\n",
    "                 'min_samples_leaf': 1,\n",
    "                 'bootstrap': False\n",
    "    }\n",
    "    \n",
    "rf = RandomForestRegressor(random_state = 42, n_jobs = -1)\n",
    "rf.set_params(**rf_params)\n",
    "rf.fit(np.concatenate((X_train, X_val), axis = 0), np.concatenate((y_train.to_numpy().ravel(), y_val.to_numpy().ravel()), axis = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "\n",
    "predictions_error = sklearn.metrics.mean_squared_error(y_test, predictions)\n",
    "\n",
    "sklearn.metrics.mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(y_test.to_numpy().ravel() - predictions)/y_test.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data = X_train, label = y_train)\n",
    "dval = xgb.DMatrix(data = X_val, label = y_val)\n",
    "dtest = xgb.DMatrix(data = X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.05,\n",
    " 'max_depth': 50,\n",
    " 'n_estimators': 1000,\n",
    " 'min_child_weight': 4,\n",
    " 'gamma': 0.2,\n",
    " 'subsample': 0.8,\n",
    " 'colsample_bytree': 0.6,\n",
    " 'lambda': 100,\n",
    " 'alpha': 0.1,\n",
    " 'objective': 'reg:squarederror',\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'rmse'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(xgb_params, dtrain, 1000, [(dval, 'val')], early_stopping_rounds = 5, maximize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_error = sklearn.metrics.mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(y_test.to_numpy().ravel() - predictions) / y_test.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = ['PM2.5', 'Pressure', 'DewPoint', 'Humidity', 'WindSpeed', 'NOx', 'O3', 'CloudCover',\n",
    "              'Month', 'SO2', 'Precipitations','Temperature', 'CO', 'Hour', 'DayWeek']\n",
    "var_list = [(1, 0.01), (1, 0.01), (1.05, 0.16), (1.9, 0.74), (2.35, 0.96), (2.4, 0.56),\n",
    "           (3.6, 0.62), (4.9, 1.94), (5.45, 1.99), (6.75, 1.72), (7, 1.8), (11.85,1.69),\n",
    "           (13.7, 1.9), (15,2.27), (15.45,2)]\n",
    "distrs = [np.random.normal(x, y,1000) for (x,y) in var_list]\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.boxplot(distrs, vert = False, whis = 1.2, showfliers = False, labels = names_list)\n",
    "plt.xlim([0, 22])\n",
    "plt.xlabel('Importance rank')\n",
    "plt.xticks(np.arange(1, 21+1, 1.0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General VS Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations = np.unique(data.StationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "r2 = []\n",
    "mae = []\n",
    "nmae = []\n",
    "predicts_list = []\n",
    "true_labels = []\n",
    "\n",
    "for station in unique_stations:\n",
    "    data_aux = data.loc[data.StationId == station]\n",
    "    X = data_aux[list(set(data_aux.columns) - set(['date_time', 'StationId', 'PM2.5']))]\n",
    "    \n",
    "    if X.shape[0] > 1000 and station != 'TN001' and station != 'KA002':\n",
    "        y = data_aux[['PM2.5']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "        rf_params = {'n_estimators': 500,\n",
    "                 'max_features': 'sqrt',\n",
    "                 'max_depth': 60,\n",
    "                 'min_samples_split': 2,\n",
    "                 'min_samples_leaf': 1,\n",
    "                 'bootstrap': False\n",
    "        }\n",
    "\n",
    "        rf = RandomForestRegressor(random_state = 42, n_jobs = -1)\n",
    "        rf.set_params(**rf_params)\n",
    "        rf.fit(np.concatenate((X_train, X_val), axis = 0), np.concatenate((y_train.to_numpy().ravel(), y_val.to_numpy().ravel()), axis = 0))\n",
    "\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        #dtrain = xgb.DMatrix(data = X_train, label = y_train)\n",
    "        #dval = xgb.DMatrix(data = X_val, label = y_val)\n",
    "        #dtest = xgb.DMatrix(data = X_test, label = y_test)\n",
    "    #\n",
    "    #\n",
    "        #xgb_params = {'eta': 0.05,\n",
    "        #     'max_depth': 50,\n",
    "        #     'n_estimators': 1000,\n",
    "        #     'min_child_weight': 4,\n",
    "        #     'gamma': 0.2,\n",
    "        #     'subsample': 0.8,\n",
    "        #     'colsample_bytree': 0.6,\n",
    "        #     'lambda': 100,\n",
    "        #     'alpha': 0.1,\n",
    "        #     'objective': 'reg:squarederror',\n",
    "        #     'seed': 42,\n",
    "        #     'eval_metric': 'rmse'}\n",
    "    #\n",
    "        #bst = xgb.train(xgb_params, dtrain, 1000, [(dval, 'val')], early_stopping_rounds = 5, maximize = False, verbose_eval = False)\n",
    "    #\n",
    "        #predictions = bst.predict(dtest)\n",
    "\n",
    "        mse.append(sklearn.metrics.mean_squared_error(y_test, predictions))\n",
    "        r2.append(sklearn.metrics.r2_score(y_test, predictions))\n",
    "        mae.append(sklearn.metrics.mean_absolute_error(y_test, predictions))\n",
    "\n",
    "        nmae.append(np.mean(np.abs(y_test.to_numpy().ravel() - predictions) / y_test.to_numpy().ravel()))\n",
    "\n",
    "        predicts_list = predicts_list + list(predictions)\n",
    "        true_labels = true_labels + [x[0] for x in y_test.values]\n",
    "\n",
    "        print(mse)\n",
    "        print(r2)\n",
    "        print(mae)\n",
    "        print(nmae)\n",
    "        print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_percentage_error(true_labels, predicts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(np.array(true_labels) - np.array(predicts_list)) / np.array(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_data = [data.loc[data.StationId == x].shape[0] for x in unique_stations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unique_stations)):\n",
    "    print('Station ' + unique_stations[i] + ' (' + str(stations_data[i]) + ' rows) : r2 = ' + str(np.round(np.array(r2[i]), 2)) +\n",
    "          ' | mae = ' + str(np.round(mae[i], 2)) + ' | rmse = ' + str(np.round(np.sqrt(mse[i]), 2))+\n",
    "         ' | nmae = ' + str(np.round(nmae[i], 2)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.sqrt(mse[i]) for i in range(len(r2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([nmae[i] for i in range(len(r2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.sqrt(mse[i]) for i in range(len(r2)) if stations_data[i] > 1000 and unique_stations[i] != 'TN001'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([nmae[i] for i in range(len(r2)) if stations_data[i] > 1000 and unique_stations[i] != 'TN001'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
